{"cells":[{"cell_type":"code","source":["# !pip install -U tfx\n","# !pip install apache-beam==2.39.0\n","# !pip install pandas-tfrecords"],"metadata":{"id":"SOibGOlEIAon"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"neIOKF3ZQ6cQ","executionInfo":{"status":"ok","timestamp":1659021848250,"user_tz":240,"elapsed":8968,"user":{"displayName":"Ani Madurkar","userId":"04951481745600804590"}},"outputId":"915d5780-fbc1-4efd-feba-6825ac0b4461"},"outputs":[{"output_type":"stream","name":"stdout","text":["Beam: 2.39.0\n","TF: 2.9.1\n","Transform: 1.9.0\n"]}],"source":["import apache_beam as beam\n","from google.protobuf import text_format\n","import math\n","import matplotlib.pyplot as plt\n","import os\n","import pandas as pd\n","import pandas_tfrecords as pdtfr\n","import pathlib\n","import pprint\n","import tempfile\n","import tensorflow as tf\n","from tensorflow_metadata.proto.v0 import schema_pb2\n","import tensorflow_transform as tft\n","import tensorflow_transform.beam as tft_beam\n","from tensorflow.python.lib.io import file_io\n","from tfx_bsl.public import tfxio\n","from tfx_bsl.coders.example_coder import RecordBatchToExamples\n","\n","# Display versions of TF and TFX related packages\n","print('Beam: {}'.format(beam.__version__))\n","print('TF: {}'.format(tf.__version__))\n","print('Transform: {}'.format(tft.__version__))"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9NuE5O9_Tpfh","executionInfo":{"status":"ok","timestamp":1659021868835,"user_tz":240,"elapsed":20597,"user":{"displayName":"Ani Madurkar","userId":"04951481745600804590"}},"outputId":"ad6daa7a-5655-4a79-b16a-424f2b29897b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Location of the data and model directory\n","DATA_DIR = '/content/drive/My Drive/Stroke Prediction ML System'\n","DATA_ROOT = f'{DATA_DIR}/data'\n","\n","# Set the paths to the reduced dataset\n","DATA_DIR_SELECT = f'{DATA_ROOT}/select'\n","TRAINING_ROOT = f'{DATA_DIR}/training'\n","TESTING_ROOT = f'{DATA_DIR}/testing'\n","SERVING_ROOT = f'{DATA_DIR}/serving'\n","\n","TRAINING_DATA = f'{TRAINING_ROOT}/stoke_prediction_training_dataset.csv'\n","TESTING_DATA = f'{TESTING_ROOT}/stoke_prediction_testing_dataset.csv'\n","SERVING_DATA = f'{SERVING_ROOT}/stoke_prediction_serving_dataset.csv'\n","\n","# We will create two pipelines. One for schema generation and one for training.\n","SCHEMA_PIPELINE_NAME = 'stroke-mlops-schema'\n","PIPELINE_NAME = 'stroke-mlops'\n","\n","# Output directory to store artifacts generated from the pipeline.\n","SCHEMA_PIPELINE_ROOT = os.path.join(DATA_DIR, 'pipeline', SCHEMA_PIPELINE_NAME)\n","PIPELINE_ROOT = os.path.join(DATA_DIR, 'pipeline', PIPELINE_NAME)\n","\n","# Path to a SQLite DB file to use as an MLMD storage.\n","SCHEMA_METADATA_PATH = os.path.join(DATA_DIR, 'metadata', SCHEMA_PIPELINE_NAME, 'metadata.db')\n","METADATA_PATH = os.path.join(DATA_DIR, 'metadata', PIPELINE_NAME, 'metadata.db')\n","\n","# Output directory where created models from the pipeline will be exported.\n","SERVING_MODEL_DIR = os.path.join(DATA_DIR, 'serving')\n","\n","# Path to curated schema file\n","SCHEMA_FOLDER = os.path.join(DATA_DIR, 'schema/schema_output')\n","\n","# Names of transformed data files\n","TRANSFORMED_TRAIN_DATA = 'train_transformed'\n","TRANSFORMED_TEST_DATA = 'test_transformed'\n","OUTPUT_DIR = f'{DATA_ROOT}/transformed'\n","\n","# Set random seed\n","RANDOM_SEED = 0"],"metadata":{"id":"hjIOYl0-TZt2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SCALE_MINMAX_FEATURE_KEYS = ['age']\n","\n","SCALE_Z_FEATURE_KEYS = ['avg_glucose_level', 'bmi']\n","\n","VOCAB_FEATURE_KEYS = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n","\n","FEATURE_KEYS = ['gender', 'age', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'avg_glucose_level', 'bmi', 'smoking_status', 'stroke']\n","\n","LABEL_KEY = 'stroke'"],"metadata":{"id":"t_0dhfQTd7e7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_schema(input_path):\n","  schema = schema_pb2.Schema()\n","  schema_text = file_io.read_file_to_string(input_path)\n","  text_format.Parse(schema_text, schema)\n","  return schema\n","\n","curated_schema = load_schema(f'{SCHEMA_FOLDER}/schema.pbtxt')"],"metadata":{"id":"DRcHbzXx7Zo8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["curated_schema"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yBhXqo9yI1pG","executionInfo":{"status":"ok","timestamp":1659021869772,"user_tz":240,"elapsed":25,"user":{"displayName":"Ani Madurkar","userId":"04951481745600804590"}},"outputId":"0d549f57-6ecb-42a5-eda4-1b2c8077dfdb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["feature {\n","  name: \"Residence_type\"\n","  type: BYTES\n","  domain: \"Residence_type\"\n","  presence {\n","    min_fraction: 1.0\n","    min_count: 1\n","  }\n","  shape {\n","    dim {\n","      size: 1\n","    }\n","  }\n","}\n","feature {\n","  name: \"age\"\n","  type: FLOAT\n","  float_domain {\n","    name: \"age\"\n","    min: 0.0\n","    max: 100.0\n","  }\n","  presence {\n","    min_fraction: 1.0\n","    min_count: 1\n","  }\n","  shape {\n","    dim {\n","      size: 1\n","    }\n","  }\n","}\n","feature {\n","  name: \"avg_glucose_level\"\n","  type: FLOAT\n","  float_domain {\n","    name: \"avg_glucose_level\"\n","    min: 25.0\n","    max: 300.0\n","  }\n","  presence {\n","    min_fraction: 1.0\n","    min_count: 1\n","  }\n","  shape {\n","    dim {\n","      size: 1\n","    }\n","  }\n","}\n","feature {\n","  name: \"bmi\"\n","  type: FLOAT\n","  float_domain {\n","    name: \"bmi\"\n","    min: 0.0\n","    max: 200.0\n","  }\n","  presence {\n","    min_fraction: 1.0\n","    min_count: 1\n","  }\n","  skew_comparator {\n","    infinity_norm {\n","      threshold: 0.01\n","    }\n","  }\n","  shape {\n","    dim {\n","      size: 1\n","    }\n","  }\n","}\n","feature {\n","  name: \"ever_married\"\n","  type: BYTES\n","  domain: \"ever_married\"\n","  presence {\n","    min_fraction: 1.0\n","    min_count: 1\n","  }\n","  shape {\n","    dim {\n","      size: 1\n","    }\n","  }\n","}\n","feature {\n","  name: \"gender\"\n","  type: BYTES\n","  domain: \"gender\"\n","  presence {\n","    min_fraction: 1.0\n","    min_count: 1\n","  }\n","  shape {\n","    dim {\n","      size: 1\n","    }\n","  }\n","}\n","feature {\n","  name: \"heart_disease\"\n","  type: INT\n","  int_domain {\n","    name: \"heart_disease\"\n","    min: 0\n","    max: 1\n","    is_categorical: true\n","  }\n","  presence {\n","    min_fraction: 1.0\n","    min_count: 1\n","  }\n","  drift_comparator {\n","    infinity_norm {\n","      threshold: 0.001\n","    }\n","  }\n","  shape {\n","    dim {\n","      size: 1\n","    }\n","  }\n","}\n","feature {\n","  name: \"hypertension\"\n","  type: INT\n","  int_domain {\n","    name: \"hypertension\"\n","    min: 0\n","    max: 1\n","    is_categorical: true\n","  }\n","  presence {\n","    min_fraction: 1.0\n","    min_count: 1\n","  }\n","  shape {\n","    dim {\n","      size: 1\n","    }\n","  }\n","}\n","feature {\n","  name: \"smoking_status\"\n","  type: BYTES\n","  domain: \"smoking_status\"\n","  presence {\n","    min_fraction: 1.0\n","    min_count: 1\n","  }\n","  shape {\n","    dim {\n","      size: 1\n","    }\n","  }\n","}\n","feature {\n","  name: \"stroke\"\n","  type: INT\n","  int_domain {\n","    name: \"stroke\"\n","    min: 0\n","    max: 1\n","    is_categorical: true\n","  }\n","  presence {\n","    min_fraction: 1.0\n","    min_count: 1\n","  }\n","  not_in_environment: \"SERVING\"\n","  shape {\n","    dim {\n","      size: 1\n","    }\n","  }\n","}\n","feature {\n","  name: \"work_type\"\n","  type: BYTES\n","  domain: \"work_type\"\n","  presence {\n","    min_fraction: 1.0\n","    min_count: 1\n","  }\n","  shape {\n","    dim {\n","      size: 1\n","    }\n","  }\n","}\n","string_domain {\n","  name: \"Residence_type\"\n","  value: \"Rural\"\n","  value: \"Urban\"\n","}\n","string_domain {\n","  name: \"ever_married\"\n","  value: \"No\"\n","  value: \"Yes\"\n","}\n","string_domain {\n","  name: \"gender\"\n","  value: \"Female\"\n","  value: \"Male\"\n","  value: \"Other\"\n","}\n","string_domain {\n","  name: \"smoking_status\"\n","  value: \"Unknown\"\n","  value: \"formerly smoked\"\n","  value: \"never smoked\"\n","  value: \"smokes\"\n","}\n","string_domain {\n","  name: \"work_type\"\n","  value: \"Govt_job\"\n","  value: \"Never_worked\"\n","  value: \"Private\"\n","  value: \"Self-employed\"\n","  value: \"children\"\n","}\n","default_environment: \"TRAINING\"\n","default_environment: \"SERVING\""]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["def preprocessing_fn(inputs):\n","  \"\"\"\n","  Preprocess input columns into transformed columns\n","  \"\"\"\n","\n","  features_dict = {}\n","\n","  for feature in SCALE_MINMAX_FEATURE_KEYS:\n","      data_col = inputs[feature] \n","      # Transform using scaling of min_max function\n","      features_dict[feature] = tft.scale_by_min_max(data_col)\n","\n","  for feature in SCALE_Z_FEATURE_KEYS:\n","      data_col = inputs[feature] \n","      # Transforming using scaling to z score\n","      features_dict[feature] = tft.scale_to_z_score(data_col)\n","\n","  for feature in VOCAB_FEATURE_KEYS:\n","      data_col = inputs[feature] \n","      # Transforming using vocabulary available in column\n","      features_dict[feature] = tft.compute_and_apply_vocabulary(data_col)\n","\n","  # No change in the label\n","  features_dict[LABEL_KEY] = inputs[LABEL_KEY]\n","\n","  return features_dict"],"metadata":{"id":"LeA9WkipUkm-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def transform_data(train_data_file, test_data_file, working_dir):\n","  \"\"\"\n","  Transform the data and write out as a TFRecord of Example protos.\n","\n","  Read in the data using the CSV reader, and transform it using a\n","  preprocessing pipeline that scales numeric data and converts categorical data\n","  from strings to int64 values indices, by creating a vocabulary for each\n","  category.\n","\n","  Args:\n","    train_data_file: File containing training data\n","    test_data_file: File containing test data\n","    working_dir: Directory to write transformed data and metadata to\n","  \"\"\"\n","\n","  with beam.Pipeline() as pipeline:\n","    with tft_beam.Context(temp_dir=DATA_ROOT):\n","      # Create a TFXIO to read the census data with the schema. To do this we\n","      # need to list all columns in order since the schema doesn't specify the\n","      # order of columns in the csv\n","      train_csv_tfxio = tfxio.CsvTFXIO(\n","          file_pattern=train_data_file,\n","          skip_header_lines=1,\n","          telemetry_descriptors=[],\n","          column_names=FEATURE_KEYS,\n","          schema=curated_schema)\n","\n","      # Read in raw data and convert using CSV TFXIO\n","      raw_data = (\n","          pipeline |\n","          'ReadTrainCsv' >> train_csv_tfxio.BeamSource())\n","\n","      # Combining data and schema into a dataset tuple.  Note that we already used\n","      # the schema to read the CSV data, but we also need it to interpret\n","      # raw_data\n","      cfg = train_csv_tfxio.TensorAdapterConfig()\n","      raw_dataset = (raw_data, cfg)\n","\n","      # The TFXIO output format is chosen for improved performance\n","      transformed_dataset, transform_fn = (\n","          raw_dataset | tft_beam.AnalyzeAndTransformDataset(\n","              preprocessing_fn, output_record_batches=True))\n","\n","      # Transformed metadata is not necessary for encoding.\n","      transformed_data, _ = transformed_dataset\n","\n","      # Extract transformed RecordBatches, encode and write them to the given\n","      # directory\n","      _ = (\n","          transformed_data\n","          | 'EncodeTrainData' >>\n","          beam.FlatMapTuple(lambda batch, _: RecordBatchToExamples(batch))\n","          | 'WriteTrainData' >> beam.io.WriteToTFRecord(\n","              os.path.join(working_dir, TRANSFORMED_TRAIN_DATA)))\n","\n","      # Now applying transform function to test data.  In this case we also \n","      # remove the header line that is present in the test data file\n","      test_csv_tfxio = tfxio.CsvTFXIO(\n","          file_pattern=test_data_file,\n","          skip_header_lines=1,\n","          telemetry_descriptors=[],\n","          column_names=FEATURE_KEYS,\n","          schema=curated_schema)\n","      \n","      raw_test_data = (\n","          pipeline\n","          | 'ReadTestCsv' >> test_csv_tfxio.BeamSource())\n","\n","      raw_test_dataset = (raw_test_data, test_csv_tfxio.TensorAdapterConfig())\n","\n","      # The TFXIO output format is chosen for improved performance\n","      transformed_test_dataset = (\n","          (raw_test_dataset, transform_fn)\n","          | tft_beam.TransformDataset(output_record_batches=True))\n","\n","      # Transformed metadata is not necessary for encoding\n","      transformed_test_data, _ = transformed_test_dataset\n","\n","      # Extract transformed RecordBatches, encode and write them to the given\n","      # directory\n","      _ = (\n","          transformed_test_data\n","          | 'EncodeTestData' >>\n","          beam.FlatMapTuple(lambda batch, _: RecordBatchToExamples(batch))\n","          | 'WriteTestData' >> beam.io.WriteToTFRecord(\n","              os.path.join(working_dir, TRANSFORMED_TEST_DATA)))\n","\n","      # Will write a SavedModel and metadata to working_dir, which can then\n","      # be read by the tft.TFTransformOutput class\n","      _ = (\n","          transform_fn\n","          | 'WriteTransformFn' >> tft_beam.WriteTransformFn(working_dir))"],"metadata":{"id":"kKeu0Lk9UkpO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform_data(TRAINING_DATA, TESTING_DATA, OUTPUT_DIR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":735},"id":"rpQool_sUkrf","executionInfo":{"status":"ok","timestamp":1659021890759,"user_tz":240,"elapsed":21000,"user":{"displayName":"Ani Madurkar","userId":"04951481745600804590"}},"outputId":"74a90eec-04a8-4749-d073-cd0a4460d372"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"]},{"output_type":"display_data","data":{"application/javascript":["\n","        if (typeof window.interactive_beam_jquery == 'undefined') {\n","          var jqueryScript = document.createElement('script');\n","          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n","          jqueryScript.type = 'text/javascript';\n","          jqueryScript.onload = function() {\n","            var datatableScript = document.createElement('script');\n","            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n","            datatableScript.type = 'text/javascript';\n","            datatableScript.onload = function() {\n","              window.interactive_beam_jquery = jQuery.noConflict(true);\n","              window.interactive_beam_jquery(document).ready(function($){\n","                \n","              });\n","            }\n","            document.head.appendChild(datatableScript);\n","          };\n","          document.head.appendChild(jqueryScript);\n","        } else {\n","          window.interactive_beam_jquery(document).ready(function($){\n","            \n","          });\n","        }"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_transform/tf_utils.py:326: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use ref() instead.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_transform/tf_utils.py:326: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use ref() instead.\n","WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/Stroke Prediction ML System/data/tftransform_tmp/35624ee710a84b31ac1cf908b41f2b6f/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/Stroke Prediction ML System/data/tftransform_tmp/35624ee710a84b31ac1cf908b41f2b6f/assets\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:struct2tensor is not available.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:struct2tensor is not available.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:tensorflow_decision_forests is not available.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:tensorflow_decision_forests is not available.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:tensorflow_text is not available.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:tensorflow_text is not available.\n","WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n","WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n","WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n","WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n","WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/Stroke Prediction ML System/data/tftransform_tmp/d52fb9a7ed8144038335208fbdec8522/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/Stroke Prediction ML System/data/tftransform_tmp/d52fb9a7ed8144038335208fbdec8522/assets\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:struct2tensor is not available.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:struct2tensor is not available.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:tensorflow_decision_forests is not available.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:tensorflow_decision_forests is not available.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:tensorflow_text is not available.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:tensorflow_text is not available.\n","WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:struct2tensor is not available.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:struct2tensor is not available.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:tensorflow_decision_forests is not available.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:tensorflow_decision_forests is not available.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:tensorflow_text is not available.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:tensorflow_text is not available.\n","WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n","WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"]}]},{"cell_type":"code","source":["tf_transform_output = tft.TFTransformOutput(OUTPUT_DIR)\n","tf_transform_output.transformed_feature_spec()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xfpLRNluqSJC","executionInfo":{"status":"ok","timestamp":1659021890760,"user_tz":240,"elapsed":16,"user":{"displayName":"Ani Madurkar","userId":"04951481745600804590"}},"outputId":"1530f186-2ebc-4fd6-d2ff-d73f326057d0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Residence_type': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None),\n"," 'age': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n"," 'avg_glucose_level': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n"," 'bmi': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n"," 'ever_married': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None),\n"," 'gender': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None),\n"," 'smoking_status': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None),\n"," 'stroke': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None),\n"," 'work_type': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None)}"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["def get_dataset(working_dir, filebase):\n","  tf_transform_output = tft.TFTransformOutput(working_dir)\n","\n","  data_path_pattern = os.path.join(working_dir, filebase + '-00000-of-00001')\n","  \n","  dataset = pdtfr.tfrecords_to_pandas(file_paths=data_path_pattern)\n","\n","  return dataset\n","\n","train_transformed_dataset = get_dataset(OUTPUT_DIR, TRANSFORMED_TRAIN_DATA)\n","train_transformed_dataset.head()"],"metadata":{"id":"F5aoRP2HqSN3","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1659021897386,"user_tz":240,"elapsed":6635,"user":{"displayName":"Ani Madurkar","userId":"04951481745600804590"}},"outputId":"aceff01f-ee5f-4b4b-be96-fa36d3a8d0a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Residence_type       age  avg_glucose_level       bmi  ever_married  \\\n","0             0.0  0.694824           2.859288  0.439707           0.0   \n","1             0.0  0.353027          -0.931756 -0.095751           0.0   \n","2             1.0  0.743652          -0.565362 -0.554715           0.0   \n","3             1.0  0.018066          -0.252118 -1.115670           1.0   \n","4             0.0  0.487305           0.230300  1.026161           0.0   \n","\n","   gender  smoking_status  stroke  work_type  \n","0     0.0             0.0     0.0        0.0  \n","1     0.0             3.0     0.0        0.0  \n","2     0.0             3.0     0.0        0.0  \n","3     1.0             1.0     0.0        2.0  \n","4     1.0             0.0     0.0        3.0  "],"text/html":["\n","  <div id=\"df-44cd5c19-0e9d-4336-8602-964ea5e0de5c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Residence_type</th>\n","      <th>age</th>\n","      <th>avg_glucose_level</th>\n","      <th>bmi</th>\n","      <th>ever_married</th>\n","      <th>gender</th>\n","      <th>smoking_status</th>\n","      <th>stroke</th>\n","      <th>work_type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.694824</td>\n","      <td>2.859288</td>\n","      <td>0.439707</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.353027</td>\n","      <td>-0.931756</td>\n","      <td>-0.095751</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.0</td>\n","      <td>0.743652</td>\n","      <td>-0.565362</td>\n","      <td>-0.554715</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>0.018066</td>\n","      <td>-0.252118</td>\n","      <td>-1.115670</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.487305</td>\n","      <td>0.230300</td>\n","      <td>1.026161</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44cd5c19-0e9d-4336-8602-964ea5e0de5c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-44cd5c19-0e9d-4336-8602-964ea5e0de5c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-44cd5c19-0e9d-4336-8602-964ea5e0de5c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":[""],"metadata":{"id":"3cgNH-R-9kQp"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"data_preprocessing.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}